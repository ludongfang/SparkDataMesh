spark {
  appName = "SparkDataFlow"
  master = "local[*]"
  
  configs {
    "spark.sql.adaptive.enabled" = "true"
    "spark.sql.adaptive.coalescePartitions.enabled" = "true"
    "spark.sql.adaptive.skewJoin.enabled" = "true"
    "spark.serializer" = "org.apache.spark.serializer.KryoSerializer"
    "spark.sql.warehouse.dir" = "/tmp/spark-warehouse"
    "spark.sql.execution.arrow.pyspark.enabled" = "true"
    "spark.sql.parquet.compression.codec" = "snappy"
    
    # Hive Support Configuration
    "spark.sql.catalogImplementation" = "hive"
    "javax.jdo.option.ConnectionURL" = "jdbc:derby:memory:metastore_db;create=true"
    "javax.jdo.option.ConnectionDriverName" = "org.apache.derby.jdbc.EmbeddedDriver"
  }
}

hive {
  # Default metastore URI (can be overridden in data contracts)
  metastoreUris = "thrift://localhost:9083"
  
  # Database configuration
  defaultDatabase = "default"
  
  # Warehouse directory
  warehouseDir = "/tmp/hive-warehouse"
  
  # Authentication
  auth {
    enabled = false
    principal = ""
    keytab = ""
  }
  
  # Connection timeout settings
  connection {
    timeoutMs = 60000
    retryIntervalMs = 5000
    maxRetries = 3
  }
}

data {
  defaultPath = "/tmp/sparkdataflow"
}

validation {
  enabled = true
}

execution {
  maxRetries = 3
  timeoutMs = 300000
}

logging {
  level = "INFO"
}